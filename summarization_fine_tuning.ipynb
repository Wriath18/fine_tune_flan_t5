{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets==2.17.0\n",
      "  Downloading datasets-2.17.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (3.12.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (1.23.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (13.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets==2.17.0)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (0.70.15)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.0)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (0.21.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets==2.17.0) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sanid\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.17.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets==2.17.0) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets==2.17.0) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets==2.17.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets==2.17.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets==2.17.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets==2.17.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.17.0) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets==2.17.0) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.17.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets==2.17.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets==2.17.0) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets==2.17.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.17.0) (1.16.0)\n",
      "Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.6/536.6 kB 17.0 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB 9.8 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: pyarrow-hotfix, fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.2.1\n",
      "    Uninstalling datasets-2.2.1:\n",
      "      Successfully uninstalled datasets-2.2.1\n",
      "Successfully installed datasets-2.17.0 fsspec-2023.10.0 pyarrow-hotfix-0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parlai 1.7.2 requires attrs~=20.2.0, but you have attrs 23.1.0 which is incompatible.\n",
      "parlai 1.7.2 requires datasets<2.2.2,>=1.4.1, but you have datasets 2.17.0 which is incompatible.\n",
      "parlai 1.7.2 requires fsspec~=2022.2.0, but you have fsspec 2023.10.0 which is incompatible.\n",
      "parlai 1.7.2 requires protobuf<=3.20.3,>=3.8.0, but you have protobuf 4.21.12 which is incompatible.\n",
      "parlai 1.7.2 requires tqdm~=4.62.1, but you have tqdm 4.66.1 which is incompatible.\n",
      "rasa 3.6.10 requires attrs<22.2,>=19.3, but you have attrs 23.1.0 which is incompatible.\n",
      "rasa 3.6.10 requires certifi>=2023.7.22, but you have certifi 2021.5.30 which is incompatible.\n",
      "rasa 3.6.10 requires jsonschema<4.18,>=3.2, but you have jsonschema 4.20.0 which is incompatible.\n",
      "rasa 3.6.10 requires networkx<2.7,>=2.4, but you have networkx 3.2.1 which is incompatible.\n",
      "rasa 3.6.10 requires packaging<21.0,>=20.0, but you have packaging 23.2 which is incompatible.\n",
      "rasa 3.6.10 requires prompt-toolkit<3.0.29,>=3.0, but you have prompt-toolkit 3.0.39 which is incompatible.\n",
      "rasa 3.6.10 requires protobuf<4.23.4,>=4.23.3, but you have protobuf 4.21.12 which is incompatible.\n",
      "rasa 3.6.10 requires SQLAlchemy<1.5.0,>=1.4.0, but you have sqlalchemy 2.0.23 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sanid\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parlai 1.7.2 requires attrs~=20.2.0, but you have attrs 23.1.0 which is incompatible.\n",
      "parlai 1.7.2 requires datasets<2.2.2,>=1.4.1, but you have datasets 2.17.0 which is incompatible.\n",
      "parlai 1.7.2 requires fsspec~=2022.2.0, but you have fsspec 2023.10.0 which is incompatible.\n",
      "parlai 1.7.2 requires protobuf<=3.20.3,>=3.8.0, but you have protobuf 4.21.12 which is incompatible.\n",
      "parlai 1.7.2 requires tqdm~=4.62.1, but you have tqdm 4.66.1 which is incompatible.\n",
      "torchaudio 2.0.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
      "torchtext 0.15.2 requires torchdata==0.6.1, but you have torchdata 0.5.1 which is incompatible.\n",
      "torchvision 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy-transformers 1.1.9 requires transformers<4.26.0,>=3.4.0, but you have transformers 4.27.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U datasets==2.17.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    evaluate==0.4.0 \\\n",
    "    rouge_score==0.1.2 \\\n",
    "    loralib==0.1.1 \\\n",
    "    peft==0.3.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"google/flan-t5-base\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(modelname, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters : 247577856\n",
      "All Parameters : 247577856\n"
     ]
    }
   ],
   "source": [
    "def number_of_trainable_params(model):\n",
    "  trainable_params = 0\n",
    "  all_params = 0\n",
    "  for _, param in model.named_parameters():\n",
    "    all_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "      trainable_params += param.numel()\n",
    "  return f\"Trainable Parameters : {trainable_params}\\nAll Parameters : {all_params}\"\n",
    "  \n",
    "print(number_of_trainable_params(original_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs['input_ids'],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens = True\n",
    ")\n",
    "seperation_line = '-'.join('' for x in range(100))\n",
    "print(seperation_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(seperation_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(seperation_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e790a6f5500b4afc8b05d8c91ccfa423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules = ['q', 'w'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters : 1769472\n",
      "All Parameters : 249347328\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(number_of_trainable_params(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_direc = f'./peft-dialogue-summary-trainaing-{str(int(time.time()))}'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir = output_direc,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate = 1e-3,\n",
    "    num_train_epochs = 1,\n",
    "    logging_steps = 1,\n",
    "    max_steps = 1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model = peft_model,\n",
    "    args = peft_training_args,\n",
    "    train_dataset = tokenized_datasets['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sanid\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a95a538241747df9232daa701a3314c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 50.25, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 8125.3963, 'train_samples_per_second': 0.001, 'train_steps_per_second': 0.0, 'train_loss': 50.25, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./peft-dialogue-summary-checkpoint-local\\\\tokenizer_config.json',\n",
       " './peft-dialogue-summary-checkpoint-local\\\\special_tokens_map.json',\n",
       " './peft-dialogue-summary-checkpoint-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "peft_model_path='./peft-dialogue-summary-checkpoint-local'\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is 58A7-18AA\n",
      "\n",
      " Directory of d:\\Programming-1\\Gen Ai\\fine_tune_flan_t5\\peft-dialogue-summary-checkpoint-local\n",
      "\n",
      "03/23/2024  04:41 PM         7,103,049 adapter_model.bin\n",
      "               1 File(s)      7,103,049 bytes\n",
      "               0 Dir(s)  420,189,810,688 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir peft-dialogue-summary-checkpoint-local\\adapter_model.bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './peft-dialogue-summary-checkpoint-local/', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters : 0\n",
      "All Parameters : 249347328\n"
     ]
    }
   ],
   "source": [
    "print(number_of_trainable_params(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ORIGINAL MODEL:\n",
      "#Person2: I'm thinking of upgrading your computer.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: #Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(seperation_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "print(seperation_line)\n",
    "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "# print(dash_line)\n",
    "# print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(seperation_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
